{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obtain a report of different ml algorithm's influence on twitter data \n",
    "\n",
    "import re # for regular expressions\n",
    "import pandas as pd \n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import string\n",
    "import nltk # for text manipulation\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline\n",
    "# from beautifultable import BeautifulTable  #report\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_tweets(filename, header = None):\n",
    "    tweet_dataset = pd.read_csv(filename, header = header)\n",
    "    tweet_dataset.columns = ['label','id','date','flag','user','text']\n",
    "    for i in ['flag','id','user','date']: \n",
    "        del tweet_dataset[i] \n",
    "    tweet_dataset.label = tweet_dataset.label.replace(4,1)\n",
    "    return tweet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = import_tweets(\"train_140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess the text in a single tweet\n",
    "def preprocess_tweet(tweet):\n",
    "    TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "    tweet = re.sub(TEXT_CLEANING_RE, ' ', str(tweet).lower())\n",
    "    #convert all urls to sting \"URL\"\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #convert all @username to \"AT_USER\"\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER', tweet)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    return tweet\n",
    "\n",
    "def tokenization(tweet):\n",
    "    tweet = ' '.join([w for w in tweet.split() if len(w)>2])\n",
    "    tokenized_tweet = tweet.split()\n",
    "    return tokenized_tweet\n",
    "\n",
    "def stemming(tokenized_tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokenized_tweet = [stemmer.stem(i) for i in tokenized_tweet]\n",
    "\n",
    "    #joining the tokenized tweet\n",
    "    stemmed_tweet = ' '.join(tokenized_tweet)\n",
    "    \n",
    "    return stemmed_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_tweets = tweets['text'].apply(preprocess_tweet)\n",
    "tokenized_tweets = processed_tweets.apply(tokenization)\n",
    "tweets['text'] = tokenized_tweets.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment this to Save tweet to a csv file\n",
    "# tweets.to_csv('text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directly load the cleaned tweets\n",
    "tweets = pd.read_csv('text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100000 Tweets from each category\n",
    "zero_tweets = tweets[tweets.label == 0].sample(n=100000, random_state=1)\n",
    "one_tweets = tweets[tweets.label == 1].sample(n=100000, random_state=1)\n",
    "tweets = pd.concat([zero_tweets, one_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "def tf_idf(tweets):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    tfidf = tfidf_vectorizer.fit_transform(tweets['text'].values.astype('U'))\n",
    "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl') \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bag of words \n",
    "\n",
    "def bow(tweets):    \n",
    "    bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    bow = bow_vectorizer.fit_transform(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to vec\n",
    "\n",
    "def word_vector(tokens, size, model_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def word2vec(tweets):\n",
    "    \n",
    "    tokenized_tweet = tweets['text'].apply(lambda x: x.split())\n",
    "    model_w2v = gensim.models.Word2Vec(\n",
    "                tokenized_tweet,\n",
    "                size=200, # desired no. of features/independent variables \n",
    "                window=5, # context window size\n",
    "                min_count=2,\n",
    "                sg = 1, # 1 for skip-gram model\n",
    "                hs = 0,\n",
    "                negative = 10, # for negative sampling\n",
    "                workers= 2, # no.of cores\n",
    "                seed = 34)\n",
    "\n",
    "    #word2vec feature set\n",
    "    wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n",
    "\n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        wordvec_arrays[i,:] = word_vector(tokenized_tweet.iloc[i], 200, model_w2v)\n",
    "\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "    \n",
    "    return wordvec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_feature_extraction(tweets, flag):\n",
    "    if flag == 1:\n",
    "        return bow(tweets)\n",
    "    elif flag == 2:\n",
    "        return tf_idf(tweets)\n",
    "    elif flag == 3:\n",
    "        return wordvec_df\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Directly Load Word2Vec data for 1.6M tweets\n",
    "features = joblib.load('wordvec_df.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter the appropriate flag number\n",
    "features = run_feature_extraction(tweets, 2)\n",
    "# Uncomment this to Save the model as a pickle in a file \n",
    "# joblib.dump(wordvec_df, 'wordvec_df_small.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data\n",
    "xtrain_data, xvalid_data, ytrain, yvalid = train_test_split(features, tweets['label'],  \n",
    "                                                            random_state=42, \n",
    "                                                            test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(xtrain_data, ytrain, flag):\n",
    "    if flag == 1:\n",
    "        model = LogisticRegression()\n",
    "        model.fit(xtrain_data, ytrain)\n",
    "    elif flag == 2:\n",
    "        model = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "        model.fit(xtrain_data, ytrain)\n",
    "    elif flag == 3:\n",
    "        print \"XG Boost\"\n",
    "        model = XGBClassifier(max_depth=5, n_estimators=2000, verbosity=2, n_jobs = 6)\n",
    "        model.fit(xtrain_data, ytrain)\n",
    "    elif flag == 4:\n",
    "        print \"Random Forest\"\n",
    "        model = RandomForestClassifier(n_estimators=500, random_state=11, verbose=1)\n",
    "        model.fit(xtrain_data, ytrain)\n",
    "    elif flag == 5:\n",
    "        print \"Gradient Boosting\"\n",
    "        model = GradientBoostingClassifier(n_estimators=1000, max_depth=5, verbose=1)\n",
    "        model.fit(xtrain_data, ytrain)\n",
    "    else:\n",
    "        return\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictions(model, xvalid_data, yvalid):\n",
    "    prediction_prob = model.predict_proba(xvalid_data)[:,1]\n",
    "    prediction_int = model.predict(xvalid_data)\n",
    "\n",
    "    print \"AUC: \"+str(roc_auc_score(yvalid, prediction_prob))\n",
    "    print \"Accuracy: \"+str(accuracy_score(yvalid, prediction_int))\n",
    "    print \"F1 score: \"+str(f1_score(yvalid, prediction_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Train the model by entering the appropriate flag number and predict\n",
    "model = train_model(xtrain_data, ytrain, 4)\n",
    "predictions(model, xvalid_data, yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a new tweet in real time\n",
    "\n",
    "def predict_realtime(tweet):\n",
    "    processed_tweet = preprocess_tweet(tweet)\n",
    "    tokens = tokenization(tweet)\n",
    "    clean_tweet = stemming(tokens)\n",
    "    tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')  \n",
    "    features = tfidf_vectorizer.transform([clean_tweet])\n",
    "    \n",
    "    load_model = joblib.load('xgb.pkl')  \n",
    "    \n",
    "    prediction_prob = load_model.predict_proba(features)[:,1]\n",
    "    prediction_int = load_model.predict(features)\n",
    "\n",
    "    if prediction_int == 0:\n",
    "        print \"Negative Tweet, Prediction Probability: \"+str(prediction_prob)\n",
    "    else:\n",
    "        print \"Positive Tweet, Prediction Probability: \"+str(prediction_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Tweet, Prediction Probability: [ 0.77577937]\n"
     ]
    }
   ],
   "source": [
    "# Enter a Tweet \n",
    "realtime_tweet = \"@ramesh Today I am feeling happy and awesome\"\n",
    "predict_realtime(realtime_tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
